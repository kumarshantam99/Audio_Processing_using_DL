{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFeatureExtractor:\n",
    "    def __init__(self, sample_rate=22050):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        \"\"\"Extract audio features including pitch, tone and pace metrics.\"\"\"\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        \n",
    "        # Pitch features\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "        pitch_mean = np.mean(pitches[pitches > 0])\n",
    "        pitch_std = np.std(pitches[pitches > 0])\n",
    "        \n",
    "        # Tone features (using spectral features)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        # Pace features\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]\n",
    "        \n",
    "        return {\n",
    "            'pitch_mean': pitch_mean,\n",
    "            'pitch_std': pitch_std,\n",
    "            'spectral_centroid_mean': np.mean(spectral_centroids),\n",
    "            'spectral_rolloff_mean': np.mean(spectral_rolloff),\n",
    "            'mfccs': mfccs.mean(axis=1),\n",
    "            'tempo': tempo\n",
    "        }\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features_list, labels):\n",
    "        self.features = torch.FloatTensor(features_list)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class AudioAnalysisModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(AudioAnalysisModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 3)  # 3 outputs: tone, pitch, pace scores\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class AudioAnalyzer:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.feature_extractor = AudioFeatureExtractor()\n",
    "        self.model = None\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "    \n",
    "    def train_model(self, train_data, train_labels, epochs=50, batch_size=32):\n",
    "        \"\"\"Train the deep learning model on extracted features.\"\"\"\n",
    "        dataset = AudioDataset(train_data, train_labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        self.model = AudioAnalysisModel(input_size=train_data.shape[1])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_features, batch_labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}')\n",
    "    \n",
    "    def analyze_audio(self, audio_path):\n",
    "        \"\"\"Analyze an audio file and return tone, pitch, and pace metrics.\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not trained or loaded\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.feature_extractor.extract_features(audio_path)\n",
    "        \n",
    "        # Prepare features for model input\n",
    "        feature_vector = np.concatenate([\n",
    "            [features['pitch_mean'], features['pitch_std'],\n",
    "             features['spectral_centroid_mean'], features['spectral_rolloff_mean'],\n",
    "             features['tempo']],\n",
    "            features['mfccs']\n",
    "        ])\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.FloatTensor(feature_vector).unsqueeze(0)\n",
    "            predictions = self.model(input_tensor)\n",
    "            tone_score, pitch_score, pace_score = predictions[0].numpy()\n",
    "        \n",
    "        return {\n",
    "            'tone_score': float(tone_score),\n",
    "            'pitch_score': float(pitch_score),\n",
    "            'pace_score': float(pace_score),\n",
    "            'raw_features': features\n",
    "        }\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the trained model.\"\"\"\n",
    "        if self.model:\n",
    "            torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load a trained model.\"\"\"\n",
    "        self.model = AudioAnalysisModel(input_size=18)  # 18 features total\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize the analyzer\n",
    "analyzer = AudioAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, Union\n",
    "\n",
    "class LabelExtractor:\n",
    "    \"\"\"Handles label extraction from different audio dataset filename formats.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Mapping for RAVDESS dataset emotions\n",
    "        self.ravdess_emotions = {\n",
    "            '01': 'neutral',\n",
    "            '02': 'calm',\n",
    "            '03': 'happy',\n",
    "            '04': 'sad',\n",
    "            '05': 'angry',\n",
    "            '06': 'fearful',\n",
    "            '07': 'disgust',\n",
    "            '08': 'surprised'\n",
    "        }\n",
    "        \n",
    "        # Mapping for RAVDESS intensity levels\n",
    "        self.ravdess_intensity = {\n",
    "            '01': 'normal',\n",
    "            '02': 'strong'\n",
    "        }\n",
    "        \n",
    "        # Mapping emotion to numerical scores for tone, pitch, and pace\n",
    "        self.emotion_scores = {\n",
    "            'neutral': {'tone': 0.5, 'pitch': 0.5, 'pace': 0.5},\n",
    "            'calm': {'tone': 0.3, 'pitch': 0.3, 'pace': 0.3},\n",
    "            'happy': {'tone': 0.8, 'pitch': 0.7, 'pace': 0.7},\n",
    "            'sad': {'tone': 0.4, 'pitch': 0.3, 'pace': 0.4},\n",
    "            'angry': {'tone': 0.8, 'pitch': 0.8, 'pace': 0.8},\n",
    "            'fearful': {'tone': 0.7, 'pitch': 0.6, 'pace': 0.7},\n",
    "            'disgust': {'tone': 0.6, 'pitch': 0.5, 'pace': 0.5},\n",
    "            'surprised': {'tone': 0.7, 'pitch': 0.8, 'pace': 0.6}\n",
    "        }\n",
    "\n",
    "    def extract_ravdess_labels(self, filename: str) -> Dict[str, Union[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract labels from RAVDESS filename format.\n",
    "        Format: modality-vocal_channel-emotion-emotional_intensity-statement-repetition-actor.wav\n",
    "        Example: 03-01-06-02-02-01-12.wav\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parts = filename.strip('.wav').split('-')\n",
    "            \n",
    "            if len(parts) != 7:\n",
    "                raise ValueError(f\"Invalid RAVDESS filename format: {filename}\")\n",
    "            \n",
    "            emotion_code = parts[2]\n",
    "            intensity_code = parts[3]\n",
    "            \n",
    "            emotion = self.ravdess_emotions.get(emotion_code, 'unknown')\n",
    "            intensity = self.ravdess_intensity.get(intensity_code, 'normal')\n",
    "            \n",
    "            # Get base scores from emotion\n",
    "            base_scores = self.emotion_scores.get(emotion, {'tone': 0.5, 'pitch': 0.5, 'pace': 0.5})\n",
    "            \n",
    "            # Adjust scores based on intensity\n",
    "            intensity_multiplier = 1.2 if intensity == 'strong' else 1.0\n",
    "            \n",
    "            return {\n",
    "                'emotion': emotion,\n",
    "                'intensity': intensity,\n",
    "                'tone_score': min(1.0, base_scores['tone'] * intensity_multiplier),\n",
    "                'pitch_score': min(1.0, base_scores['pitch'] * intensity_multiplier),\n",
    "                'pace_score': min(1.0, base_scores['pace'] * intensity_multiplier)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing RAVDESS filename {filename}: {str(e)}\")\n",
    "\n",
    "    def extract_tess_labels(self, filename: str) -> Dict[str, Union[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract labels from TESS filename format.\n",
    "        Format: OAF_emotion_word.wav or YAF_emotion_word.wav\n",
    "        Example: OAF_angry_word.wav\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract emotion from filename\n",
    "            match = re.search(r'[OY]AF_(\\w+)_', filename)\n",
    "            if not match:\n",
    "                raise ValueError(f\"Invalid TESS filename format: {filename}\")\n",
    "            \n",
    "            emotion = match.group(1).lower()\n",
    "            \n",
    "            # Map TESS emotion to scores (using same mapping as RAVDESS for consistency)\n",
    "            scores = self.emotion_scores.get(emotion, {'tone': 0.5, 'pitch': 0.5, 'pace': 0.5})\n",
    "            \n",
    "            return {\n",
    "                'emotion': emotion,\n",
    "                'tone_score': scores['tone'],\n",
    "                'pitch_score': scores['pitch'],\n",
    "                'pace_score': scores['pace']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing TESS filename {filename}: {str(e)}\")\n",
    "\n",
    "    def extract_custom_labels(self, filename: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract labels from custom filename format.\n",
    "        Format: tone_X_pitch_Y_pace_Z.wav\n",
    "        Example: tone_0.8_pitch_0.6_pace_0.7.wav\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract scores from filename using regex\n",
    "            tone_match = re.search(r'tone_(0?\\.\\d+)', filename)\n",
    "            pitch_match = re.search(r'pitch_(0?\\.\\d+)', filename)\n",
    "            pace_match = re.search(r'pace_(0?\\.\\d+)', filename)\n",
    "            \n",
    "            if not all([tone_match, pitch_match, pace_match]):\n",
    "                raise ValueError(f\"Invalid custom filename format: {filename}\")\n",
    "            \n",
    "            return {\n",
    "                'tone_score': float(tone_match.group(1)),\n",
    "                'pitch_score': float(pitch_match.group(1)),\n",
    "                'pace_score': float(pace_match.group(1))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing custom filename {filename}: {str(e)}\")\n",
    "\n",
    "    def extract_labels(self, filename: str, dataset_type: str = 'auto') -> Dict[str, Union[str, float]]:\n",
    "        \"\"\"\n",
    "        Main function to extract labels from filename based on dataset type.\n",
    "        \"\"\"\n",
    "        filename = os.path.basename(filename)\n",
    "        \n",
    "        if dataset_type == 'auto':\n",
    "            # Try to automatically determine dataset type from filename\n",
    "            if re.match(r'\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}\\.wav', filename):\n",
    "                dataset_type = 'ravdess'\n",
    "            elif re.match(r'[OY]AF_\\w+_\\w+\\.wav', filename):\n",
    "                dataset_type = 'tess'\n",
    "            elif re.match(r'tone_[\\d.]+_pitch_[\\d.]+_pace_[\\d.]+\\.wav', filename):\n",
    "                dataset_type = 'custom'\n",
    "            else:\n",
    "                raise ValueError(f\"Could not automatically determine dataset type for: {filename}\")\n",
    "        \n",
    "        # Extract labels based on dataset type\n",
    "        if dataset_type == 'ravdess':\n",
    "            return self.extract_ravdess_labels(filename)\n",
    "        elif dataset_type == 'tess':\n",
    "            return self.extract_tess_labels(filename)\n",
    "        elif dataset_type == 'custom':\n",
    "            return self.extract_custom_labels(filename)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset type: {dataset_type}\")\n",
    "\n",
    "def extract_label_from_filename(filename: str, dataset_type: str = 'auto') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Wrapper function to extract labels and return them in the format expected by the model.\n",
    "    Returns a numpy array of [tone_score, pitch_score, pace_score]\n",
    "    \"\"\"\n",
    "    extractor = LabelExtractor()\n",
    "    labels = extractor.extract_labels(filename, dataset_type)\n",
    "    return np.array([\n",
    "        labels['tone_score'],\n",
    "        labels['pitch_score'],\n",
    "        labels['pace_score']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm  # for progress tracking\n",
    "\n",
    "def prepare_dataset(dataset_path, analyzer):\n",
    "    features_list = []\n",
    "    labels = []\n",
    "    \n",
    "    # Walk through the dataset directory\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in tqdm(files):\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extract features\n",
    "                try:\n",
    "                    features = analyzer.feature_extractor.extract_features(file_path)\n",
    "                    feature_vector = np.concatenate([\n",
    "                        [features['pitch_mean'], features['pitch_std'],\n",
    "                         features['spectral_centroid_mean'], features['spectral_rolloff_mean'],\n",
    "                         features['tempo']],\n",
    "                        features['mfccs']\n",
    "                    ])\n",
    "                    features_list.append(feature_vector)\n",
    "                    \n",
    "                    # Get label from filename or metadata\n",
    "                    # This will depend on the specific dataset structure\n",
    "                    label = extract_label_from_filename(file)  # You'll need to implement this\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    return np.array(features_list), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]C:\\Users\\kshantam\\AppData\\Local\\Temp\\ipykernel_27668\\2798333181.py:22: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]\n",
      "100%|██████████| 60/60 [00:35<00:00,  1.67it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.41it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.33it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.68it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 24.59it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 23.32it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 24.83it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 24.51it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 26.53it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.89it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 28.45it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 24.57it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 27.92it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 22.48it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.83it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.08it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 26.67it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.27it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 24.47it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 24.88it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 23.53it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.71it/s]\n",
      "100%|██████████| 60/60 [00:02<00:00, 25.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "features, labels = prepare_dataset(\"data/Audio_Speech_Actors_01-24/\",analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_audio_analysis(results, scale_to_10=True):\n",
    "    \"\"\"\n",
    "    Interprets audio analysis results and prints them in a human-readable format.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): The analysis results from the AudioAnalyzer\n",
    "        scale_to_10 (bool): If True, scales scores from 0-1 to 1-10\n",
    "    \"\"\"\n",
    "    def scale_score(score):\n",
    "        \"\"\"Scales score from 0-1 to 1-10\"\"\"\n",
    "        return round(score * 9 + 1, 1)\n",
    "    \n",
    "    def get_tone_description(score):\n",
    "        if score >= 8.5: return \"Exceptionally clear and pure\"\n",
    "        elif score >= 7: return \"Very clear and clean\"\n",
    "        elif score >= 5.5: return \"Moderately clear\"\n",
    "        elif score >= 4: return \"Slightly unclear\"\n",
    "        else: return \"Unclear or noisy\"\n",
    "        \n",
    "    def get_pitch_description(score):\n",
    "        if score >= 8.5: return \"Very high\"\n",
    "        elif score >= 7: return \"High\"\n",
    "        elif score >= 5.5: return \"Medium-high\"\n",
    "        elif score >= 4: return \"Medium-low\"\n",
    "        else: return \"Low\"\n",
    "        \n",
    "    def get_pace_description(score):\n",
    "        if score >= 8.5: return \"Very fast\"\n",
    "        elif score >= 7: return \"Fast\"\n",
    "        elif score >= 5.5: return \"Moderate to fast\"\n",
    "        elif score >= 4: return \"Moderate\"\n",
    "        else: return \"Slow\"\n",
    "    \n",
    "    # Scale scores if requested\n",
    "    if scale_to_10:\n",
    "        tone = scale_score(results['tone_score'])\n",
    "        pitch = scale_score(results['pitch_score'])\n",
    "        pace = scale_score(results['pace_score'])\n",
    "    else:\n",
    "        tone = results['tone_score']\n",
    "        pitch = results['pitch_score']\n",
    "        pace = results['pace_score']\n",
    "    \n",
    "    # Get raw features for additional context\n",
    "    raw = results['raw_features']\n",
    "    \n",
    "    # Format the interpretation\n",
    "    print(\"=== Audio Analysis Results ===\")\n",
    "    print(f\"\\nScores (1-10 scale):\")\n",
    "    print(f\"Tone Quality: {tone}/10 - {get_tone_description(tone)}\")\n",
    "    print(f\"Pitch Level: {pitch}/10 - {get_pitch_description(pitch)}\")\n",
    "    print(f\"Pace/Tempo: {pace}/10 - {get_pace_description(pace)}\")\n",
    "    \n",
    "    print(\"\\nDetailed Measurements:\")\n",
    "    print(f\"Average Pitch: {raw['pitch_mean']:.1f} Hz\")\n",
    "    print(f\"Pitch Variation: ±{raw['pitch_std']:.1f} Hz\")\n",
    "    print(f\"Tempo: {raw['tempo']:.1f} BPM\")\n",
    "    \n",
    "    print(\"\\nSuggested Classification:\")\n",
    "    if raw['pitch_mean'] > 1500:\n",
    "        voice_range = \"Very high frequency content\"\n",
    "    elif raw['pitch_mean'] > 800:\n",
    "        voice_range = \"High frequency content\"\n",
    "    elif raw['pitch_mean'] > 300:\n",
    "        voice_range = \"Mid-range frequency content\"\n",
    "    else:\n",
    "        voice_range = \"Low frequency content\"\n",
    "    \n",
    "    print(f\"- {voice_range}\")\n",
    "    print(f\"- {'High' if raw['tempo'] > 100 else 'Moderate' if raw['tempo'] > 70 else 'Low'} tempo\")\n",
    "    print(f\"- {'High' if tone > 7 else 'Moderate' if tone > 4 else 'Low'} clarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.4012\n",
      "Epoch [20/50], Loss: 0.1936\n",
      "Epoch [30/50], Loss: 0.1129\n",
      "Epoch [40/50], Loss: 0.0926\n",
      "Epoch [50/50], Loss: 0.0830\n",
      "{'tone_score': 1.0694383382797241, 'pitch_score': 0.7080138921737671, 'pace_score': 0.6949702501296997, 'raw_features': {'pitch_mean': np.float32(1922.2175), 'pitch_std': np.float32(1117.2708), 'spectral_centroid_mean': np.float64(2815.4766442985297), 'spectral_rolloff_mean': np.float64(5268.95947265625), 'mfccs': array([-572.27435  ,   33.545666 ,  -15.067412 ,   -3.5327373,\n",
      "        -12.325186 ,  -10.237376 ,  -14.086486 ,  -13.915781 ,\n",
      "         -6.127895 ,   -4.269773 ,   -9.902204 ,   -3.2373993,\n",
      "         -6.0632944], dtype=float32), 'tempo': np.float64(117.45383522727273)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshantam\\AppData\\Local\\Temp\\ipykernel_27668\\2798333181.py:22: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]\n"
     ]
    }
   ],
   "source": [
    "# 4. Train the model\n",
    "analyzer.train_model(features, labels, epochs=50)\n",
    "\n",
    "# 5. Save the trained model\n",
    "analyzer.save_model(\"audio_analysis_model.pth\")\n",
    "\n",
    "# 6. Analyze a new audio file\n",
    "results = analyzer.analyze_audio(\"test_data/Actor_24/03-01-08-01-01-02-24.wav\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tone_score': 0.3368957042694092, 'pitch_score': 0.3546028435230255, 'pace_score': 0.23705440759658813, 'raw_features': {'pitch_mean': np.float32(2000.1384), 'pitch_std': np.float32(1145.5349), 'spectral_centroid_mean': np.float64(3595.106462082038), 'spectral_rolloff_mean': np.float64(6334.114193488024), 'mfccs': array([-730.6705   ,   38.349815 ,    2.6743178,    9.384072 ,\n",
      "         -5.97962  ,   -3.0296078,  -11.018359 ,   -9.151866 ,\n",
      "         -7.464394 ,   -5.854757 ,   -5.191709 ,   -2.5191557,\n",
      "         -7.5516753], dtype=float32), 'tempo': np.float64(135.99917763157896)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshantam\\AppData\\Local\\Temp\\ipykernel_27668\\2798333181.py:22: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 6. Analyze a new audio file\n",
    "results = analyzer.analyze_audio(\"test_data/Actor_24/03-01-02-01-02-01-24.wav\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Audio Analysis Results ===\n",
      "\n",
      "Scores (1-10 scale):\n",
      "Tone Quality: 4.0/10 - Slightly unclear\n",
      "Pitch Level: 4.2/10 - Medium-low\n",
      "Pace/Tempo: 3.1/10 - Slow\n",
      "\n",
      "Detailed Measurements:\n",
      "Average Pitch: 2000.1 Hz\n",
      "Pitch Variation: ±1145.5 Hz\n",
      "Tempo: 136.0 BPM\n",
      "\n",
      "Suggested Classification:\n",
      "- Very high frequency content\n",
      "- High tempo\n",
      "- Low clarity\n"
     ]
    }
   ],
   "source": [
    "interpret_audio_analysis(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
